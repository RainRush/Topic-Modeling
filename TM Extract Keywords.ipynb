{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TM Extract Keywords\n",
    "\n",
    "- Date: 2019/08/30\n",
    "- Author: Daniel Hu (University of Melbourne)\n",
    "- Description: Assist the researchers coding the topics by highlighting the keywords of topics in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries and Collect Data\n",
    "## (1) Import Packages and Prepare Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in python console\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing\n",
    "## (1) Remove Noisy Characters & Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafile_to_list(data_file):\n",
    "    # Convert to list\n",
    "    data_list = data_file.content.values.tolist()\n",
    "    return data_list\n",
    "    \n",
    "def remove_noise(data):\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', doc) for doc in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', doc) for doc in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", doc) for doc in data]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def doc_to_words(docs):\n",
    "    for doc in docs:\n",
    "        yield(gensim.utils.simple_preprocess(str(doc), deacc=True))\n",
    "\n",
    "data_list = datafile_to_list(df)\n",
    "data = remove_noise(data_list)\n",
    "data_words = list(doc_to_words(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Building Topic Model\n",
    "## (1) Build the LDA Topic Model & View the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.141*\"team\" + 0.140*\"game\" + 0.081*\"play\" + 0.080*\"sale\" + 0.033*\"nhl\" + '\n",
      "  '0.029*\"trade\" + 0.029*\"cd\" + 0.018*\"ice\" + 0.014*\"detroit\" + 0.014*\"joe\"'),\n",
      " (1,\n",
      "  '0.086*\"pin\" + 0.044*\"processor\" + 0.044*\"character\" + 0.040*\"font\" + '\n",
      "  '0.034*\"mirror\" + 0.018*\"radius\" + 0.018*\"quran\" + 0.017*\"stephen\" + '\n",
      "  '0.014*\"ford\" + 0.012*\"alot\"'),\n",
      " (2,\n",
      "  '0.048*\"notice\" + 0.040*\"material\" + 0.037*\"signal\" + 0.037*\"external\" + '\n",
      "  '0.030*\"circuit\" + 0.022*\"case_western\" + 0.022*\"reserve_university\" + '\n",
      "  '0.021*\"oil\" + 0.018*\"charle\" + 0.016*\"william\"'),\n",
      " (3,\n",
      "  '0.054*\"not\" + 0.034*\"do\" + 0.028*\"would\" + 0.026*\"be\" + 0.021*\"say\" + '\n",
      "  '0.020*\"think\" + 0.019*\"know\" + 0.017*\"go\" + 0.016*\"people\" + 0.015*\"get\"'),\n",
      " (4,\n",
      "  '0.084*\"library\" + 0.061*\"object\" + 0.045*\"cub\" + 0.011*\"static\" + '\n",
      "  '0.008*\"compiler\" + 0.008*\"void\" + 0.006*\"borland\" + 0.006*\"bc\" + '\n",
      "  '0.003*\"sps\" + 0.001*\"initialize\"'),\n",
      " (5,\n",
      "  '0.112*\"israel\" + 0.061*\"israeli\" + 0.045*\"jew\" + 0.038*\"arab\" + '\n",
      "  '0.037*\"jewish\" + 0.027*\"peace\" + 0.024*\"bomb\" + 0.023*\"islam\" + '\n",
      "  '0.023*\"muslim\" + 0.015*\"attack\"'),\n",
      " (6,\n",
      "  '0.069*\"drive\" + 0.036*\"card\" + 0.029*\"driver\" + 0.028*\"mac\" + 0.021*\"cpu\" + '\n",
      "  '0.019*\"memory\" + 0.018*\"chip\" + 0.016*\"machine\" + 0.016*\"board\" + '\n",
      "  '0.015*\"scsi\"'),\n",
      " (7,\n",
      "  '0.767*\"ax\" + 0.058*\"max\" + 0.006*\"icon\" + 0.004*\"film\" + 0.003*\"plot\" + '\n",
      "  '0.003*\"catalog\" + 0.003*\"download\" + 0.002*\"bmp\" + 0.002*\"atom\" + '\n",
      "  '0.002*\"wallpaper\"'),\n",
      " (8,\n",
      "  '0.053*\"faq\" + 0.036*\"ed\" + 0.025*\"plane\" + 0.023*\"st\" + 0.023*\"brown\" + '\n",
      "  '0.022*\"dangerous\" + 0.021*\"description\" + 0.021*\"rob\" + 0.020*\"dual\" + '\n",
      "  '0.018*\"failure\"'),\n",
      " (9,\n",
      "  '0.028*\"system\" + 0.020*\"car\" + 0.019*\"use\" + 0.018*\"new\" + 0.016*\"computer\" '\n",
      "  '+ 0.015*\"buy\" + 0.015*\"need\" + 0.013*\"type\" + 0.013*\"price\" + 0.013*\"high\"'),\n",
      " (10,\n",
      "  '0.051*\"god\" + 0.032*\"evidence\" + 0.025*\"christian\" + 0.022*\"reason\" + '\n",
      "  '0.018*\"believe\" + 0.017*\"claim\" + 0.016*\"exist\" + 0.015*\"faith\" + '\n",
      "  '0.015*\"sense\" + 0.013*\"bible\"'),\n",
      " (11,\n",
      "  '0.042*\"robert\" + 0.028*\"pa\" + 0.028*\"illinoi\" + 0.027*\"channel\" + '\n",
      "  '0.027*\"benefit\" + 0.026*\"crash\" + 0.025*\"statistic\" + 0.025*\"joseph\" + '\n",
      "  '0.025*\"van\" + 0.022*\"link\"'),\n",
      " (12,\n",
      "  '0.051*\"recommend\" + 0.045*\"gateway\" + 0.042*\"usenet\" + 0.042*\"michigan\" + '\n",
      "  '0.039*\"warranty\" + 0.027*\"bank\" + 0.026*\"meg\" + 0.024*\"probe\" + '\n",
      "  '0.024*\"plug\" + 0.018*\"sony\"'),\n",
      " (13,\n",
      "  '0.034*\"window\" + 0.032*\"file\" + 0.031*\"program\" + 0.025*\"use\" + '\n",
      "  '0.020*\"software\" + 0.019*\"information\" + 0.016*\"copy\" + 0.015*\"code\" + '\n",
      "  '0.015*\"available\" + 0.015*\"version\"'),\n",
      " (14,\n",
      "  '0.019*\"time\" + 0.017*\"year\" + 0.014*\"first\" + 0.013*\"may\" + 0.010*\"back\" + '\n",
      "  '0.010*\"also\" + 0.010*\"day\" + 0.008*\"number\" + 0.008*\"call\" + 0.008*\"case\"'),\n",
      " (15,\n",
      "  '0.087*\"internet\" + 0.071*\"bike\" + 0.060*\"server\" + 0.048*\"md\" + '\n",
      "  '0.046*\"engine\" + 0.038*\"ride\" + 0.035*\"route\" + 0.031*\"dod\" + '\n",
      "  '0.029*\"announcement\" + 0.021*\"zone\"'),\n",
      " (16,\n",
      "  '0.018*\"gun\" + 0.018*\"state\" + 0.017*\"law\" + 0.017*\"government\" + '\n",
      "  '0.017*\"people\" + 0.015*\"kill\" + 0.012*\"death\" + 0.012*\"american\" + '\n",
      "  '0.010*\"child\" + 0.010*\"country\"'),\n",
      " (17,\n",
      "  '0.088*\"line\" + 0.080*\"organization\" + 0.043*\"write\" + 0.040*\"article\" + '\n",
      "  '0.033*\"university\" + 0.030*\"host\" + 0.019*\"reply\" + 0.018*\"nntp_poste\" + '\n",
      "  '0.016*\"thank\" + 0.016*\"nntp_posting\"'),\n",
      " (18,\n",
      "  '0.066*\"space\" + 0.019*\"earth\" + 0.018*\"mount\" + 0.016*\"launch\" + '\n",
      "  '0.015*\"moon\" + 0.015*\"research\" + 0.015*\"mission\" + 0.014*\"orbit\" + '\n",
      "  '0.014*\"nasa\" + 0.012*\"satellite\"'),\n",
      " (19,\n",
      "  '0.148*\"key\" + 0.042*\"encryption\" + 0.036*\"public\" + 0.036*\"security\" + '\n",
      "  '0.033*\"clipper\" + 0.033*\"chip\" + 0.026*\"government\" + 0.025*\"secure\" + '\n",
      "  '0.024*\"tap\" + 0.021*\"pgp\"')]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -14.150704488362106\n",
      "\n",
      "Coherence Score:  0.4993186780826996\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Simplify Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide Topic and Document to Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number = 11000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### (1) Data File to List & Remove Noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (Brian Yamauchi) Subject: DC-X: Choice of a New Generation (was Re: '\n",
      " 'SSRT Roll-Out Speech) Organization: Case Western Reserve University Lines: '\n",
      " '27 Distribution: world NNTP-Posting-Host: yuggoth.ces.cwru.edu In-reply-to: '\n",
      " 'message of 21 Apr 1993 22:09:32 -0400 In article (Jordan Katz) writes: > '\n",
      " 'Speech Delivered by Col. Simon P. Worden, > The Deputy for Technology, SDIO '\n",
      " '> > Most of you, as am I, are \"children of the 1960s.\" We grew >up in an age '\n",
      " 'of miracles -- Inter-Continental Ballistic Missiles, >nuclear energy, '\n",
      " 'computers, flights to the moon. But these were >miracles of our parents '\n",
      " 'doing. > Speech by Pete Worden > Delivered Before the U.S. Space Foundation '\n",
      " 'Conference > Im embarrassed when my generation is compared with the last '\n",
      " '>generation -- the giants of the last great space era, the 1950s >and 1960s. '\n",
      " 'They went to the moon - we built a telescope that >cant see straight. They '\n",
      " 'soft-landed on Mars - the least we >could do is soft-land on Earth! Just out '\n",
      " 'of curiousity, how old is Worden? -- '\n",
      " '_______________________________________________________________________________ '\n",
      " 'Brian Yamauchi Case Western Reserve University Department of Computer '\n",
      " 'Engineering and Science '\n",
      " '_______________________________________________________________________________ ']\n"
     ]
    }
   ],
   "source": [
    "doc_data = datafile_to_list(df)[doc_number]\n",
    "doc = remove_noise([doc_data])\n",
    "pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Split Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (Brian Yamauchi) Subject: DC-X: Choice of a New Generation (was Re: '\n",
      " 'SSRT Roll-Out Speech) Organization: Case Western Reserve University Lines: '\n",
      " '27 Distribution: world NNTP-Posting-Host: yuggoth.ces.cwru.edu In-reply-to: '\n",
      " 'message of 21 Apr 1993 22:09:32 -0400 In article (Jordan Katz) writes: > '\n",
      " 'Speech Delivered by Col. Simon P. Worden, > The Deputy for Technology, SDIO '\n",
      " '> > Most of you, as am I, are \"children of the 1960s.\"',\n",
      " 'We grew >up in an age of miracles -- Inter-Continental Ballistic Missiles, '\n",
      " '>nuclear energy, computers, flights to the moon.',\n",
      " 'But these were >miracles of our parents doing.',\n",
      " '> Speech by Pete Worden > Delivered Before the U.S. Space Foundation '\n",
      " 'Conference > Im embarrassed when my generation is compared with the last '\n",
      " '>generation -- the giants of the last great space era, the 1950s >and 1960s.',\n",
      " 'They went to the moon - we built a telescope that >cant see straight.',\n",
      " 'They soft-landed on Mars - the least we >could do is soft-land on Earth!',\n",
      " 'Just out of curiousity, how old is Worden?',\n",
      " '-- '\n",
      " '_______________________________________________________________________________ '\n",
      " 'Brian Yamauchi Case Western Reserve University Department of Computer '\n",
      " 'Engineering and Science '\n",
      " '_______________________________________________________________________________']\n"
     ]
    }
   ],
   "source": [
    "# Generate the Sentence List\n",
    "sent_list = sent_tokenize(doc[0])\n",
    "pprint(sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Generate Bigram List by Tokenized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brian_yamauchi',\n",
      " 'case_western',\n",
      " 'reserve_university',\n",
      " 'distribution_world',\n",
      " 'nntp_posting',\n",
      " 'brian_yamauchi',\n",
      " 'case_western',\n",
      " 'reserve_university']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the doc to tokens(words)\n",
    "doc_words = list(doc_to_words(doc))\n",
    "\n",
    "# Remove Stop Words\n",
    "doc_words_nostops = remove_stopwords(doc_words)\n",
    "\n",
    "# Form Bigrams\n",
    "doc_words_bigrams = make_bigrams(doc_words_nostops)\n",
    "\n",
    "# Generate Bigram List for this document\n",
    "bigram_list = [bigram for bigram in doc_words_bigrams[0] if '_' in bigram]\n",
    "pprint(bigram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Check Topic Distribution for This Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The topic distribution of this document is:\n",
      "Topic No. 17 \t 0.22\n",
      "Topic No. 14 \t 0.14\n",
      "Topic No. 18 \t 0.14\n",
      "Topic No. 3 \t 0.13\n",
      "Topic No. 9 \t 0.12\n",
      "Topic No. 10 \t 0.11\n",
      "Topic No. 2 \t 0.05\n",
      "Topic No. 16 \t 0.03\n",
      "Topic No. 8 \t 0.01\n",
      "Topic No. 13 \t 0.01\n",
      "Topic No. 6 \t 0.01\n",
      "Topic No. 0 \t 0.00\n",
      "Topic No. 11 \t 0.00\n",
      "Topic No. 15 \t 0.00\n",
      "Topic No. 5 \t 0.00\n",
      "Topic No. 19 \t 0.00\n",
      "Topic No. 12 \t 0.00\n",
      "Topic No. 1 \t 0.00\n",
      "Topic No. 7 \t 0.00\n",
      "Topic No. 4 \t 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThe topic distribution of this document is:\")\n",
    "topic_list = sorted(lda_model.get_document_topics(corpus[doc_number],minimum_probability=0.0), key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"Topic No.\", topic_list[i][0], \"\\t\", \"%.2f\" % topic_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Decide Topic, Generate Keyword List & Highlight the Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords of topic 17\n",
      "['line',\n",
      " 'organization',\n",
      " 'write',\n",
      " 'article',\n",
      " 'university',\n",
      " 'host',\n",
      " 'reply',\n",
      " 'nntp_poste',\n",
      " 'thank',\n",
      " 'nntp_posting',\n",
      " 'get',\n",
      " 'anyone',\n",
      " 'post',\n",
      " 'mail',\n",
      " 'help',\n",
      " 'win',\n",
      " 'good',\n",
      " 'look',\n",
      " 'distribution_world',\n",
      " 'news']\n",
      "\u001b[1;30mSentence 1 \u001b[0mFrom \u001b[0m: \u001b[0m( \u001b[0mBrian \u001b[0mYamauchi \u001b[0m) \u001b[0mSubject \u001b[0m: \u001b[0mDC-X \u001b[0m: \u001b[0mChoice \u001b[0mof \u001b[0ma \u001b[0mNew \u001b[0mGeneration \u001b[0m( \u001b[0mwas \u001b[0mRe \u001b[0m: \u001b[0mSSRT \u001b[0mRoll-Out \u001b[0mSpeech \u001b[0m) \u001b[1;31mOrganization \u001b[0m: \u001b[0mCase \u001b[0mWestern \u001b[0mReserve \u001b[1;31mUniversity \u001b[1;31mLines \u001b[0m: \u001b[0m27 \u001b[1;31mDistribution \u001b[0m: \u001b[1;31mworld \u001b[0mNNTP-Posting-Host \u001b[0m: \u001b[0myuggoth.ces.cwru.edu \u001b[0mIn-reply-to \u001b[0m: \u001b[0mmessage \u001b[0mof \u001b[0m21 \u001b[0mApr \u001b[0m1993 \u001b[0m22:09:32 \u001b[0m-0400 \u001b[0mIn \u001b[1;31marticle \u001b[0m( \u001b[0mJordan \u001b[0mKatz \u001b[0m) \u001b[1;31mwrites \u001b[0m: \u001b[0m> \u001b[0mSpeech \u001b[0mDelivered \u001b[0mby \u001b[0mCol. \u001b[0mSimon \u001b[0mP. \u001b[0mWorden \u001b[0m, \u001b[0m> \u001b[0mThe \u001b[0mDeputy \u001b[0mfor \u001b[0mTechnology \u001b[0m, \u001b[0mSDIO \u001b[0m> \u001b[0m> \u001b[0mMost \u001b[0mof \u001b[0myou \u001b[0m, \u001b[0mas \u001b[0mam \u001b[0mI \u001b[0m, \u001b[0mare \u001b[0m`` \u001b[0mchildren \u001b[0mof \u001b[0mthe \u001b[0m1960s \u001b[0m. \u001b[0m''\n",
      "\u001b[1;30mSentence 2 \u001b[0mWe \u001b[0mgrew \u001b[0m> \u001b[0mup \u001b[0min \u001b[0man \u001b[0mage \u001b[0mof \u001b[0mmiracles \u001b[0m-- \u001b[0mInter-Continental \u001b[0mBallistic \u001b[0mMissiles \u001b[0m, \u001b[0m> \u001b[0mnuclear \u001b[0menergy \u001b[0m, \u001b[0mcomputers \u001b[0m, \u001b[0mflights \u001b[0mto \u001b[0mthe \u001b[0mmoon \u001b[0m.\n",
      "\u001b[1;30mSentence 3 \u001b[0mBut \u001b[0mthese \u001b[0mwere \u001b[0m> \u001b[0mmiracles \u001b[0mof \u001b[0mour \u001b[0mparents \u001b[0mdoing \u001b[0m.\n",
      "\u001b[1;30mSentence 4 \u001b[0m> \u001b[0mSpeech \u001b[0mby \u001b[0mPete \u001b[0mWorden \u001b[0m> \u001b[0mDelivered \u001b[0mBefore \u001b[0mthe \u001b[0mU.S. \u001b[0mSpace \u001b[0mFoundation \u001b[0mConference \u001b[0m> \u001b[0mIm \u001b[0membarrassed \u001b[0mwhen \u001b[0mmy \u001b[0mgeneration \u001b[0mis \u001b[0mcompared \u001b[0mwith \u001b[0mthe \u001b[0mlast \u001b[0m> \u001b[0mgeneration \u001b[0m-- \u001b[0mthe \u001b[0mgiants \u001b[0mof \u001b[0mthe \u001b[0mlast \u001b[0mgreat \u001b[0mspace \u001b[0mera \u001b[0m, \u001b[0mthe \u001b[0m1950s \u001b[0m> \u001b[0mand \u001b[0m1960s \u001b[0m.\n",
      "\u001b[1;30mSentence 5 \u001b[0mThey \u001b[0mwent \u001b[0mto \u001b[0mthe \u001b[0mmoon \u001b[0m- \u001b[0mwe \u001b[0mbuilt \u001b[0ma \u001b[0mtelescope \u001b[0mthat \u001b[0m> \u001b[0mcant \u001b[0msee \u001b[0mstraight \u001b[0m.\n",
      "\u001b[1;30mSentence 6 \u001b[0mThey \u001b[0msoft-landed \u001b[0mon \u001b[0mMars \u001b[0m- \u001b[0mthe \u001b[0mleast \u001b[0mwe \u001b[0m> \u001b[0mcould \u001b[0mdo \u001b[0mis \u001b[0msoft-land \u001b[0mon \u001b[0mEarth \u001b[0m!\n",
      "\u001b[1;30mSentence 7 \u001b[0mJust \u001b[0mout \u001b[0mof \u001b[0mcuriousity \u001b[0m, \u001b[0mhow \u001b[0mold \u001b[0mis \u001b[0mWorden \u001b[0m?\n",
      "\u001b[1;30mSentence 8 \u001b[0m-- \u001b[0m_______________________________________________________________________________ \u001b[0mBrian \u001b[0mYamauchi \u001b[0mCase \u001b[0mWestern \u001b[0mReserve \u001b[1;31mUniversity \u001b[0mDepartment \u001b[0mof \u001b[0mComputer \u001b[0mEngineering \u001b[0mand \u001b[0mScience \u001b[0m_______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def gen_keyword_list(topic_number):\n",
    "    keywords = [word for word, prop in lda_model.show_topic(topic_number, topn=20)] # keyword list of topic j\n",
    "    return keywords\n",
    "\n",
    "topic_number = 17\n",
    "\n",
    "keyword_list = gen_keyword_list(topic_number)\n",
    "print(\"Keywords of topic\", topic_number)\n",
    "pprint(keyword_list)\n",
    "\n",
    "highlight_list = []\n",
    "for i, sent in enumerate(sent_list):\n",
    "    bigram_flag = False\n",
    "    tokens = gensim.utils.simple_preprocess(str(sent), deacc=False)\n",
    "    for j, token in enumerate(tokens):\n",
    "        # Skip the stop_words\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Check Bigram\n",
    "        if bigram_flag == True:\n",
    "            to_bigram = tokens[j-1] + '_' + token\n",
    "            if to_bigram in bigram_list:\n",
    "                if to_bigram in keyword_list and to_bigram not in highlight_list:\n",
    "                    highlight_list.append(tokens[j-1])\n",
    "                    highlight_list.append(token)\n",
    "        \n",
    "        bigram_flag = False\n",
    "        if any(token in bigram for bigram in bigram_list):\n",
    "            bigram_flag = True\n",
    "\n",
    "        lemma_list = lemmatization([[token]], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        for lemma in lemma_list[0]:\n",
    "            if lemma in keyword_list and lemma not in highlight_list:\n",
    "                highlight_list.append(token)\n",
    "    \n",
    "    this_sent = \"\\033[1;30mSentence \" + str(i+1)\n",
    "    for word in word_tokenize(sent):\n",
    "        if word.lower() in highlight_list:\n",
    "            this_sent += ' ' + \"\\033[1;31m\" + word\n",
    "        else:\n",
    "            this_sent += ' ' + \"\\033[0m\" + word\n",
    "    print(this_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
