{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TM Predict New Document\n",
    "\n",
    "- Date: 2019/08/31\n",
    "- Author: Daniel Hu (University of Melbourne)\n",
    "- Description: Assist the researchers by predicting the code of this document / sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries and Collect Data\n",
    "## (1) Import Packages and Prepare Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in python console\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Import Data and Datafile to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df.head()\n",
    "\n",
    "def datafile_to_list(data_file):\n",
    "    # Convert to list\n",
    "    data_list = data_file.content.values.tolist()\n",
    "    return data_list\n",
    "\n",
    "data_list = datafile_to_list(df)\n",
    "shuffle(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Seperating Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to build LDA model 1 (100%)\n",
    "len_nintyfive_per = int(len(data_list)*0.95)\n",
    "\n",
    "# Used to build LDA model 2 (90%)\n",
    "train_data_list = data_list[:len_nintyfive_per]\n",
    "\n",
    "# Load into LDA model 2 one by one (do not corrupt model 2 by saving in a new model)\n",
    "test_data_list = data_list[len_nintyfive_per:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing\n",
    "## (1) Remove Noisy Characters & Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(data):\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', doc) for doc in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', doc) for doc in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", doc) for doc in data]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def doc_to_words(docs):\n",
    "    for doc in docs:\n",
    "        yield(gensim.utils.simple_preprocess(str(doc), deacc=True))\n",
    "        \n",
    "data = remove_noise(data_list)\n",
    "data_words = list(doc_to_words(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Building Topic Model for All Data\n",
    "## (1) Build the LDA Topic Model & View the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.077*\"bike\" + 0.045*\"insurance\" + 0.042*\"logic\" + 0.037*\"fear\" + '\n",
      "  '0.034*\"dod\" + 0.033*\"click\" + 0.033*\"ride\" + 0.032*\"advice\" + '\n",
      "  '0.031*\"straight\" + 0.026*\"islam\"'),\n",
      " (1,\n",
      "  '0.113*\"count\" + 0.041*\"united_states\" + 0.040*\"april\" + 0.037*\"best\" + '\n",
      "  '0.031*\"vote\" + 0.027*\"faqs\" + 0.027*\"planet\" + 0.019*\"percent\" + '\n",
      "  '0.018*\"june\" + 0.018*\"andrew\"'),\n",
      " (2,\n",
      "  '0.094*\"game\" + 0.061*\"team\" + 0.052*\"win\" + 0.046*\"play\" + 0.035*\"year\" + '\n",
      "  '0.033*\"player\" + 0.024*\"league\" + 0.019*\"lose\" + 0.019*\"division\" + '\n",
      "  '0.018*\"fan\"'),\n",
      " (3,\n",
      "  '0.820*\"ax\" + 0.058*\"max\" + 0.007*\"fool\" + 0.005*\"stephen\" + 0.004*\"xv\" + '\n",
      "  '0.004*\"florida\" + 0.002*\"sigh\" + 0.001*\"mb\" + 0.000*\"part\" + 0.000*\"end\"'),\n",
      " (4,\n",
      "  '0.166*\"gun\" + 0.062*\"moon\" + 0.060*\"firearm\" + 0.048*\"handgun\" + '\n",
      "  '0.040*\"weapon\" + 0.035*\"space_shuttle\" + 0.028*\"gm\" + 0.015*\"tank\" + '\n",
      "  '0.013*\"safety\" + 0.013*\"ban\"'),\n",
      " (5,\n",
      "  '0.065*\"pittsburgh\" + 0.035*\"amp\" + 0.027*\"gordon_banks\" + 0.026*\"surrender\" '\n",
      "  '+ 0.021*\"intellect\" + 0.020*\"oakland\" + 0.019*\"shameful\" + 0.019*\"chastity\" '\n",
      "  '+ 0.017*\"soon\" + 0.015*\"univ\"'),\n",
      " (6,\n",
      "  '0.042*\"key\" + 0.022*\"government\" + 0.018*\"system\" + 0.015*\"american\" + '\n",
      "  '0.013*\"national\" + 0.013*\"private\" + 0.012*\"security\" + 0.012*\"encryption\" '\n",
      "  '+ 0.012*\"flag\" + 0.010*\"provide\"'),\n",
      " (7,\n",
      "  '0.043*\"child\" + 0.030*\"war\" + 0.029*\"kill\" + 0.025*\"israel\" + '\n",
      "  '0.024*\"attack\" + 0.022*\"israeli\" + 0.020*\"jew\" + 0.020*\"people\" + '\n",
      "  '0.016*\"land\" + 0.015*\"jewish\"'),\n",
      " (8,\n",
      "  '0.053*\"widget\" + 0.030*\"window\" + 0.027*\"use\" + 0.025*\"system\" + '\n",
      "  '0.019*\"run\" + 0.017*\"work\" + 0.017*\"software\" + 0.017*\"problem\" + '\n",
      "  '0.014*\"bit\" + 0.014*\"color\"'),\n",
      " (9,\n",
      "  '0.058*\"law\" + 0.046*\"right\" + 0.038*\"public\" + 0.035*\"state\" + '\n",
      "  '0.026*\"crime\" + 0.025*\"encrypt\" + 0.024*\"property\" + 0.020*\"shall\" + '\n",
      "  '0.018*\"people\" + 0.014*\"rifle\"'),\n",
      " (10,\n",
      "  '0.053*\"file\" + 0.031*\"application\" + 0.029*\"resource\" + 0.021*\"program\" + '\n",
      "  '0.021*\"string\" + 0.020*\"space\" + 0.019*\"value\" + 0.017*\"display\" + '\n",
      "  '0.017*\"available\" + 0.017*\"image\"'),\n",
      " (11,\n",
      "  '0.022*\"may\" + 0.011*\"make\" + 0.011*\"would\" + 0.010*\"many\" + 0.010*\"case\" + '\n",
      "  '0.010*\"question\" + 0.009*\"point\" + 0.009*\"mean\" + 0.009*\"use\" + '\n",
      "  '0.009*\"also\"'),\n",
      " (12,\n",
      "  '0.102*\"god\" + 0.047*\"christian\" + 0.032*\"believe\" + 0.032*\"bible\" + '\n",
      "  '0.028*\"religion\" + 0.024*\"sin\" + 0.024*\"false\" + 0.024*\"faith\" + '\n",
      "  '0.024*\"church\" + 0.022*\"say\"'),\n",
      " (13,\n",
      "  '0.037*\"biblical\" + 0.028*\"assault\" + 0.027*\"teaching\" + 0.023*\"hunt\" + '\n",
      "  '0.018*\"hair\" + 0.009*\"middle_east\" + 0.009*\"news_gateway\" + '\n",
      "  '0.008*\"inquisition\" + 0.008*\"utexas\" + 0.008*\"cow\"'),\n",
      " (14,\n",
      "  '0.077*\"core\" + 0.027*\"tend\" + 0.021*\"conclusion\" + 0.020*\"moral\" + '\n",
      "  '0.020*\"usage\" + 0.019*\"foundation\" + 0.019*\"berkeley\" + 0.019*\"english\" + '\n",
      "  '0.019*\"language\" + 0.018*\"meaning\"'),\n",
      " (15,\n",
      "  '0.048*\"not\" + 0.030*\"do\" + 0.021*\"be\" + 0.020*\"get\" + 0.019*\"go\" + '\n",
      "  '0.015*\"say\" + 0.014*\"think\" + 0.014*\"know\" + 0.014*\"would\" + 0.014*\"time\"'),\n",
      " (16,\n",
      "  '0.072*\"line\" + 0.062*\"organization\" + 0.039*\"write\" + 0.033*\"article\" + '\n",
      "  '0.028*\"university\" + 0.025*\"host\" + 0.015*\"visual\" + 0.015*\"reply\" + '\n",
      "  '0.014*\"nntp_posting\" + 0.014*\"thank\"'),\n",
      " (17,\n",
      "  '0.037*\"car\" + 0.035*\"sell\" + 0.031*\"cost\" + 0.026*\"buy\" + 0.026*\"sale\" + '\n",
      "  '0.024*\"register\" + 0.023*\"price\" + 0.021*\"model\" + 0.019*\"drive\" + '\n",
      "  '0.018*\"low\"'),\n",
      " (18,\n",
      "  '0.067*\"food\" + 0.031*\"xdm\" + 0.027*\"eternal\" + 0.025*\"cold\" + '\n",
      "  '0.024*\"belong\" + 0.023*\"politic\" + 0.023*\"eat\" + 0.020*\"utah\" + '\n",
      "  '0.020*\"tune\" + 0.019*\"fatal\"'),\n",
      " (19,\n",
      "  '0.042*\"ranger\" + 0.038*\"double\" + 0.037*\"matthew\" + 0.031*\"jack\" + '\n",
      "  '0.023*\"rick\" + 0.021*\"ford\" + 0.019*\"radar_detector\" + 0.016*\"namely\" + '\n",
      "  '0.015*\"taylor\" + 0.013*\"good_luck\"')]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -14.314281254734093\n",
      "\n",
      "Coherence Score:  0.5236575485943878\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Gather the Label of the Last 10% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Hand code the similarity\n",
    "# Remember the purpose of this experiment is to analyse the precision and recall of this method\n",
    "#\n",
    "# This experiment does not work. \n",
    "# (1) The topics only focus on topics like no. 0, 6, and 17.\n",
    "# (2) The topic id will be different for each attempt, so might need to hand code the topics for \n",
    "#     the \"all_data\" and \"train_data\" id labels to match\n",
    "# (3) Even taking 10% of the data may cause change to the topic (consistency issue)\n",
    "###\n",
    "\n",
    "# Record doc_id and its most dominant topic\n",
    "# len(train_data_lemma)\n",
    "# for doc_number in range(len(train_data_lemma),len(data_lemmatized)):\n",
    "#     topic_list = sorted(lda_model.get_document_topics(corpus[doc_number],minimum_probability=0.0), key=lambda x: (x[1]), reverse=True)\n",
    "#     if topic_list[0][0] == 6:\n",
    "#         print(\"Topic No.\", topic_list[1][0], topic_list[1][1])\n",
    "#     else:\n",
    "#         print(\"Topic No.\", topic_list[0][0], topic_list[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Apply Training Data to Build Model\n",
    "## (1) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lemma = data_lemmatized[:len_nintyfive_per]\n",
    "test_data_lemma = data_lemmatized[len_nintyfive_per:]\n",
    "\n",
    "# Create Dictionary\n",
    "train_id2word = corpora.Dictionary(train_data_lemma)\n",
    "\n",
    "# Create Corpus\n",
    "train_texts = train_data_lemma\n",
    "\n",
    "# Term Document Frequency\n",
    "train_corpus = [id2word.doc2bow(text) for text in train_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.085*\"sale\" + 0.058*\"bike\" + 0.039*\"keyboard\" + 0.036*\"ride\" + '\n",
      "  '0.036*\"count\" + 0.034*\"unit\" + 0.034*\"tape\" + 0.033*\"fear\" + 0.027*\"dod\" + '\n",
      "  '0.027*\"advice\"'),\n",
      " (1,\n",
      "  '0.094*\"car\" + 0.056*\"buy\" + 0.054*\"price\" + 0.045*\"sell\" + 0.032*\"pay\" + '\n",
      "  '0.031*\"cost\" + 0.023*\"model\" + 0.020*\"money\" + 0.018*\"drive\" + '\n",
      "  '0.017*\"dealer\"'),\n",
      " (2,\n",
      "  '0.033*\"go\" + 0.026*\"get\" + 0.022*\"year\" + 0.017*\"time\" + 0.017*\"good\" + '\n",
      "  '0.014*\"not\" + 0.013*\"day\" + 0.013*\"s\" + 0.013*\"back\" + 0.012*\"first\"'),\n",
      " (3,\n",
      "  '0.134*\"game\" + 0.118*\"team\" + 0.071*\"play\" + 0.070*\"win\" + 0.032*\"hockey\" + '\n",
      "  '0.032*\"season\" + 0.025*\"goal\" + 0.024*\"roger\" + 0.022*\"baseball\" + '\n",
      "  '0.020*\"stat\"'),\n",
      " (4,\n",
      "  '0.086*\"god\" + 0.055*\"christian\" + 0.033*\"believe\" + 0.032*\"bible\" + '\n",
      "  '0.028*\"church\" + 0.027*\"religion\" + 0.025*\"belief\" + 0.024*\"faith\" + '\n",
      "  '0.020*\"jesus\" + 0.018*\"exist\"'),\n",
      " (5,\n",
      "  '0.872*\"ax\" + 0.060*\"max\" + 0.004*\"mono\" + 0.001*\"xv\" + 0.001*\"mb\" + '\n",
      "  '0.000*\"part\" + 0.000*\"reply\" + 0.000*\"university\" + 0.000*\"qax\" + '\n",
      "  '0.000*\"qq\"'),\n",
      " (6,\n",
      "  '0.044*\"red\" + 0.038*\"clinton\" + 0.034*\"european\" + 0.033*\"health\" + '\n",
      "  '0.030*\"drug\" + 0.028*\"volume\" + 0.026*\"doug\" + 0.025*\"hot\" + '\n",
      "  '0.022*\"education\" + 0.020*\"case_western\"'),\n",
      " (7,\n",
      "  '0.062*\"armenian\" + 0.051*\"russian\" + 0.022*\"child\" + 0.021*\"agency\" + '\n",
      "  '0.019*\"woman\" + 0.018*\"fbi\" + 0.018*\"natural\" + 0.017*\"turk\" + '\n",
      "  '0.017*\"muslim\" + 0.015*\"greek\"'),\n",
      " (8,\n",
      "  '0.038*\"window\" + 0.023*\"drive\" + 0.019*\"card\" + 0.019*\"problem\" + '\n",
      "  '0.019*\"run\" + 0.017*\"driver\" + 0.016*\"use\" + 0.016*\"mac\" + 0.016*\"ship\" + '\n",
      "  '0.015*\"machine\"'),\n",
      " (9,\n",
      "  '0.019*\"gun\" + 0.018*\"government\" + 0.016*\"state\" + 0.015*\"law\" + '\n",
      "  '0.013*\"right\" + 0.013*\"people\" + 0.013*\"military\" + 0.012*\"american\" + '\n",
      "  '0.010*\"public\" + 0.009*\"control\"'),\n",
      " (10,\n",
      "  '0.072*\"probe\" + 0.042*\"star\" + 0.042*\"multiple\" + 0.035*\"input\" + '\n",
      "  '0.031*\"wave\" + 0.024*\"element\" + 0.023*\"venus\" + 0.021*\"bs\" + 0.021*\"gif\" + '\n",
      "  '0.020*\"hook\"'),\n",
      " (11,\n",
      "  '0.106*\"war\" + 0.055*\"attack\" + 0.050*\"israel\" + 0.047*\"jew\" + '\n",
      "  '0.040*\"british\" + 0.036*\"israeli\" + 0.033*\"jewish\" + 0.025*\"religious\" + '\n",
      "  '0.024*\"sea\" + 0.022*\"peace\"'),\n",
      " (12,\n",
      "  '0.054*\"pressure\" + 0.022*\"senator\" + 0.017*\"taylor\" + 0.014*\"relativity\" + '\n",
      "  '0.005*\"pork\" + 0.004*\"mark_wilson\" + 0.001*\"manufacturing_atlanta\" + '\n",
      "  '0.001*\"someone_elses\" + 0.000*\"aint_charity\" + 0.000*\"money_wilson\"'),\n",
      " (13,\n",
      "  '0.051*\"final\" + 0.050*\"january\" + 0.042*\"boston\" + 0.029*\"jeff\" + '\n",
      "  '0.028*\"giant\" + 0.022*\"bat\" + 0.022*\"prediction\" + 0.021*\"jam\" + '\n",
      "  '0.017*\"writer\" + 0.015*\"forum\"'),\n",
      " (14,\n",
      "  '0.076*\"chip\" + 0.031*\"orbit\" + 0.029*\"signal\" + 0.028*\"ground\" + '\n",
      "  '0.023*\"damage\" + 0.022*\"hole\" + 0.021*\"te\" + 0.021*\"gas\" + 0.020*\"pin\" + '\n",
      "  '0.019*\"cycle\"'),\n",
      " (15,\n",
      "  '0.044*\"united_states\" + 0.035*\"request\" + 0.033*\"tar\" + 0.031*\"surface\" + '\n",
      "  '0.024*\"edge\" + 0.023*\"terminal\" + 0.021*\"market\" + 0.019*\"dec\" + '\n",
      "  '0.018*\"nsa\" + 0.016*\"publish\"'),\n",
      " (16,\n",
      "  '0.107*\"line\" + 0.097*\"organization\" + 0.050*\"write\" + 0.047*\"article\" + '\n",
      "  '0.047*\"university\" + 0.039*\"host\" + 0.022*\"reply\" + 0.022*\"nntp_poste\" + '\n",
      "  '0.021*\"nntp_posting\" + 0.019*\"thank\"'),\n",
      " (17,\n",
      "  '0.130*\"key\" + 0.048*\"encryption\" + 0.030*\"clipper\" + 0.028*\"cd\" + '\n",
      "  '0.027*\"clipper_chip\" + 0.026*\"security\" + 0.026*\"switch\" + '\n",
      "  '0.026*\"algorithm\" + 0.022*\"wire\" + 0.019*\"privacy\"'),\n",
      " (18,\n",
      "  '0.032*\"file\" + 0.017*\"system\" + 0.016*\"space\" + 0.015*\"include\" + '\n",
      "  '0.015*\"available\" + 0.015*\"program\" + 0.013*\"information\" + 0.011*\"use\" + '\n",
      "  '0.011*\"also\" + 0.010*\"source\"'),\n",
      " (19,\n",
      "  '0.034*\"not\" + 0.023*\"would\" + 0.022*\"do\" + 0.015*\"be\" + 0.014*\"know\" + '\n",
      "  '0.014*\"say\" + 0.014*\"make\" + 0.013*\"think\" + 0.011*\"may\" + 0.010*\"write\"')]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "train_lda_model = gensim.models.ldamodel.LdaModel(corpus=train_corpus,\n",
    "                                                  id2word=train_id2word,\n",
    "                                                  num_topics=20, \n",
    "                                                  random_state=100,\n",
    "                                                  update_every=1,\n",
    "                                                  chunksize=100,\n",
    "                                                  passes=10,\n",
    "                                                  alpha='auto',\n",
    "                                                  per_word_topics=True)\n",
    "\n",
    "pprint(train_lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -14.216960468199522\n",
      "\n",
      "Coherence Score:  0.47801838837242494\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', train_lda_model.log_perplexity(train_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "train_coherence_model_lda = CoherenceModel(model=train_lda_model, texts=train_data_lemma, dictionary=train_id2word, coherence='c_v')\n",
    "train_coherence_lda = train_coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', train_coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Predict New Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document No. 4\n",
      "From: gsnow@clark.edu (Gary Snow)\n",
      "Subject: Re: WARNING! Don't break Powerbook screen\n",
      "Article-I.D.: clark.1993Apr6.210853.26502\n",
      "Organization: Clark College, Vancouver, Wa.  USA\n",
      "Lines: 20\n",
      "\n",
      "In article <D2150035.ub9c68@outpost.SF-Bay.org> peirce@outpost.SF-Bay.org (Michael Peirce) writes:\n",
      ">\n",
      ">Surprised? Shouldn't be.  Protective tarriffs almost always end up\n",
      ">hurting the U.S. in the long run.  Same with subsidies.  they way\n",
      ">to build a strong economy isn't to wall it off from the tough outside\n",
      ">world, but rather to compete in the global market place (and don't\n",
      ">come crying when the world doesn't always want to play by our house\n",
      ">rules).\n",
      "\n",
      "Tell that to the Japanese, their local market is neatly protected by\n",
      "the Japanese government. Its one very tough nut to crack. In fact\n",
      "the only current way to break into it, is to do it with a Japanese\n",
      "company as a partner in the venture.\n",
      " \n",
      "Gary\n",
      "\n",
      "-- \n",
      "-----\n",
      "Gary Snow\n",
      "uunet!clark!gsnow  or  gsnow@clark.edu\n",
      "\n",
      "The topic distribution of this new document is:\n",
      "Topic No. 19 \tProb. 0.39\n",
      "Topic No. 2 \tProb. 0.34\n",
      "Topic No. 16 \tProb. 0.09\n",
      "Topic No. 9 \tProb. 0.05\n",
      "Topic No. 15 \tProb. 0.04\n",
      "Topic No. 14 \tProb. 0.03\n",
      "Topic No. 8 \tProb. 0.02\n",
      "Topic No. 18 \tProb. 0.02\n"
     ]
    }
   ],
   "source": [
    "test_doc_no = 4\n",
    "\n",
    "# Create Dictionary\n",
    "test_this_id2word = corpora.Dictionary([test_data_lemma[test_doc_no]])\n",
    "\n",
    "# Create Corpus\n",
    "test_this_texts = [test_data_lemma[test_doc_no]]\n",
    "\n",
    "# Term Document Frequency\n",
    "test_this_corpus = [train_id2word.doc2bow(text) for text in test_this_texts]\n",
    "\n",
    "unseen_doc = test_this_corpus[0]\n",
    "vector = train_lda_model[unseen_doc]\n",
    "\n",
    "print(\"Document No.\", test_doc_no)\n",
    "print(test_data_list[test_doc_no])\n",
    "\n",
    "print(\"The topic distribution of this new document is:\")\n",
    "for (topic_id, percentage) in sorted(vector[0], key=lambda x: (x[1]), reverse=True):\n",
    "    print(\"Topic No.\", topic_id, \"\\tProb.\", \"%.2f\"%percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyhu/anaconda3/envs/topicmodel/lib/python3.6/site-packages/gensim/models/ldamodel.py:824: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    }
   ],
   "source": [
    "train_lda_model.update(test_this_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Predict New Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Preprocessing the Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sentence = \"They soft-landed on Mars - the least we could do is soft-land on Earth!\"\n",
    "sent_in_list = remove_noise([pre_sentence])\n",
    "sentence_words = list(doc_to_words(sent_in_list))\n",
    "\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "sent_bigram = gensim.models.Phrases(sentence_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "sent_bigram_mod = gensim.models.phrases.Phraser(sent_bigram)\n",
    "\n",
    "# Remove Stop Words\n",
    "sent_words_nostops = remove_stopwords(sentence_words)\n",
    "\n",
    "# Form Bigrams\n",
    "sent_words_bigrams = make_bigrams(sent_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "sent_lemma = lemmatization(sent_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Update Word Dictionary and Predict the Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: They soft-landed on Mars - the least we could do is soft-land on Earth!\n",
      "The topic distribution of this sentence is:\n",
      "Topic No. 19 \tProb. 0.39\n",
      "Topic No. 18 \tProb. 0.15\n",
      "Topic No. 16 \tProb. 0.10\n",
      "Topic No. 2 \tProb. 0.10\n",
      "Topic No. 11 \tProb. 0.09\n",
      "Topic No. 9 \tProb. 0.06\n",
      "Topic No. 8 \tProb. 0.03\n",
      "Topic No. 1 \tProb. 0.01\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "this_sent_id2word = corpora.Dictionary([sent_lemma[0]])\n",
    "\n",
    "# Create Corpus\n",
    "this_sent_texts = [sent_lemma[0]]\n",
    "\n",
    "# Term Document Frequency\n",
    "this_sent_corpus = [train_id2word.doc2bow(text) for text in this_sent_texts]\n",
    "\n",
    "unseen_doc = this_sent_corpus[0]\n",
    "vector = train_lda_model[unseen_doc]\n",
    "\n",
    "\n",
    "print(\"Sentence:\", pre_sentence)\n",
    "\n",
    "print(\"The topic distribution of this sentence is:\")\n",
    "for (topic_id, percentage) in sorted(vector[0], key=lambda x: (x[1]), reverse=True):\n",
    "    print(\"Topic No.\", topic_id, \"\\tProb.\", \"%.2f\"%percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
